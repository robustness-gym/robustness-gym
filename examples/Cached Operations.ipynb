{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bita92610e06c5d4297ab8f7e1346a063b8",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness_gym import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "from nlp import list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Checking /Users/krandiash/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py for additional imports.\nLock 140652529281488 acquired on /Users/krandiash/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock\nFound main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq\nFound specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8\nFound script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py to /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.py\nFound dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/dataset_infos.json to /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/dataset_infos.json\nFound metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.json\nLock 140652529281488 released on /Users/krandiash/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock\nUsing custom data configuration default\nLoading Dataset Infos from /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8\nOverwrite dataset info from restored data version.\nLoading Dataset info from /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8\nReusing dataset boolq (/Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8)\nConstructing Dataset for split train[:1%], from /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8\nAll the checksums matched successfully for post processing resources\nCaching processed dataset at /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/cache-2bd08e6234bf8ef5138502dc954c6a7b.arrow\n100%|██████████| 94/94 [00:00<00:00, 12176.18it/s]\nDone writing 94 examples in 52708 bytes /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/tmp70k5xg48.\n"
    }
   ],
   "source": [
    "# Load a few examples from the boolq dataset\n",
    "dataset = Dataset.load_dataset('boolq', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset(features: {'answer': Value(dtype='bool', id=None), 'index': Value(dtype='string', id=None), 'passage': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None)}, num_rows: 94, num_slices: 0)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Caching processed dataset at cache-26181064159052232777117048075389735138573308848957013099079102149242.arrow\n100%|██████████| 3/3 [00:00<00:00, 22.13it/s]\nDone writing 94 examples in 158257 bytes /Users/krandiash/Desktop/workspace/projects/robustness-gym/examples/tmp_z5y07xr.\n"
    }
   ],
   "source": [
    "# Return a dataset which caches the outputs of the operations applied\n",
    "dataset = dataset.stow(\n",
    "    cached_ops={\n",
    "        # Apply Spacy separately to only the \"question\" key\n",
    "        Spacy(): [['question']],\n",
    "        # Apply the StripText operation to the \"question\" key and the \"passage\" key separately\n",
    "        StripText(): [['question'], ['passage']]\n",
    "    },\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_keys(['answer', 'index', 'passage', 'question', 'cache'])\ndict_keys(['Spacy', 'StripText'])\ndict_keys(['question'])\ndict_keys(['passage', 'question'])\n"
    }
   ],
   "source": [
    "print(dataset[0].keys())\n",
    "print(dataset[0]['cache'].keys())\n",
    "print(dataset[0]['cache']['Spacy'].keys())\n",
    "print(dataset[0]['cache']['StripText'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_cached_op = CachedOperation(identifier='MyOp',  \n",
    "                                apply_fn=lambda text_batch_a, text_batch_b: [fuzz.ratio(text_a, text_b) for text_a, text_b in zip(text_batch_a, text_batch_b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Caching processed dataset at cache-1728072295086098938381636053840329254782020419366196562308104870007.arrow\n100%|██████████| 3/3 [00:00<00:00, 111.95it/s]\nDone writing 94 examples in 159009 bytes /Users/krandiash/Desktop/workspace/projects/robustness-gym/examples/tmp55fwjiuo.\n"
    }
   ],
   "source": [
    "dataset = dataset.stow(cached_ops={\n",
    "    my_cached_op: [['question', 'passage']],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'[\"question\", \"passage\"]': 13}\n"
    }
   ],
   "source": [
    "# The outputs are cached, as expected\n",
    "print(dataset[0]['cache']['MyOp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['question', 'passage'] 13\n"
    }
   ],
   "source": [
    "# The keys are JSON-dumped, so easy to load back in\n",
    "for keys, value in dataset[0]['cache']['MyOp'].items():\n",
    "    print(json.loads(keys), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}