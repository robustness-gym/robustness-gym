{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bita92610e06c5d4297ab8f7e1346a063b8",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness_gym import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "from transformers import *\n",
    "from nlp import list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Checking /Users/krandiash/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py for additional imports.\nLock 140439929096896 acquired on /Users/krandiash/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock\nFound main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq\nFound specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8\nFound script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py to /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.py\nUpdating dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/dataset_infos.json to /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/dataset_infos.json\nFound metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/boolq/boolq.py at /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8/boolq.json\nLock 140439929096896 released on /Users/krandiash/.cache/huggingface/datasets/494f4f058948bc745482f513922716c8bff83799bb20c7026e85a0026b643401.8462e3fa824dc775b18d0c33eb06f61781c1157028a832923a3b9547ee06f9c2.py.lock\nUsing custom data configuration default\nLoading Dataset Infos from /usr/local/anaconda3/envs/mayanshell/lib/python3.8/site-packages/nlp/datasets/boolq/f9eaaad11e850927735893d5fba78b4064e30569b9c0337facd7e35f6c0697a8\nOverwrite dataset info from restored data version.\nLoading Dataset info from /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0\nReusing dataset boolq (/Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0)\nConstructing Dataset for split train[:1%], from /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0\nLoading cached processed dataset at /Users/krandiash/.cache/huggingface/datasets/boolq/default/0.1.0/cache-757a075d9f0cff930f69e7700d516af5.arrow\n"
    }
   ],
   "source": [
    "# Load a few examples from the boolq dataset\n",
    "dataset = Dataset.load_dataset('boolq', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset(schema: {'question': 'string', 'answer': 'bool', 'passage': 'string', 'index': 'string'}, num_rows: 94, num_slices: 0)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Caching processed dataset at cache-21673943725506890527705415222262047092225530433934578056354105899629.arrow\n100%|██████████| 3/3 [00:00<00:00,  7.61it/s]\nDone writing 94 examples in 158257 bytes cache-21673943725506890527705415222262047092225530433934578056354105899629.arrow.\n"
    }
   ],
   "source": [
    "# Return a dataset which caches the outputs of the operations applied\n",
    "dataset = dataset.stow(\n",
    "    cached_ops={\n",
    "        # Apply Spacy separately to only the \"question\" key\n",
    "        Spacy(): [['question']],\n",
    "        # Apply the StripText operation to the \"question\" key and the \"passage\" key separately\n",
    "        StripText(): [['question'], ['passage']]\n",
    "    },\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_keys(['question', 'answer', 'passage', 'index', 'cache'])\ndict_keys(['Spacy', 'StripText'])\ndict_keys(['question'])\ndict_keys(['passage', 'question'])\n"
    }
   ],
   "source": [
    "print(dataset[0].keys())\n",
    "print(dataset[0]['cache'].keys())\n",
    "print(dataset[0]['cache']['Spacy'].keys())\n",
    "print(dataset[0]['cache']['StripText'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_cached_op = CachedOperation(identifier='MyOp',  \n",
    "                                apply_fn=lambda text_batch_a, text_batch_b: [fuzz.ratio(text_a, text_b) for text_a, text_b in zip(text_batch_a, text_batch_b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Caching processed dataset at cache-25842721568737739515085703747208608204532832045481796505764655886292.arrow\n100%|██████████| 3/3 [00:00<00:00, 50.15it/s]\nDone writing 94 examples in 159009 bytes cache-25842721568737739515085703747208608204532832045481796505764655886292.arrow.\n"
    }
   ],
   "source": [
    "dataset = dataset.stow(cached_ops={\n",
    "    my_cached_op: [['question', 'passage']],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'[\"question\", \"passage\"]': 13}\n"
    }
   ],
   "source": [
    "# The outputs are cached, as expected\n",
    "print(dataset[0]['cache']['MyOp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['question', 'passage'] 13\n"
    }
   ],
   "source": [
    "# The keys are JSON-dumped, so easy to load back in\n",
    "for keys, value in dataset[0]['cache']['MyOp'].items():\n",
    "    print(json.loads(keys), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}