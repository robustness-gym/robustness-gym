{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import load_dataset\n",
    "dataset = load_dataset('wikipedia', '20200501.en')\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_auto.py:791: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Kalitta',\n",
       " 'text': 'Kalitta may refer to:\\n\\nConnie Kalitta (born 1938), a retired American drag racer and CEO of the eponymous Kallita Air.\\nDoug Kalitta (born 1964), an American drag racer, nephew of Connie Kalitta and owner of Kalitta Charters.\\nScott Kalitta (1962-2008), an American drag racer and son of Connie Kalitta.\\nKalitta Air, a cargo airline flying Boeing 747 aircraft.\\nKalitta Charters, a cargo airline flying medium-sized aircraft.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Salesforce is a tech company with headquarter in California\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# data_collections = []\n",
    "# for data in tqdm(dataset):\n",
    "#     if \"salesforce\" in data[\"text\"].lower():\n",
    "#         data_collections.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.save(data_collections, \"/export/home/Experiments/20200701/data_collections.tmp\")\n",
    "data_collections = torch.load(\"/export/home/Experiments/20200701/data_collections.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why did it fail when feeding text directly?\n",
    "# dataset = dataset.map(lambda entry, index: tokenize_to_features(entry, index), with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "block_size = tokenizer.max_len - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "\n",
    "class LazyTextDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        instance = self.dataset[i]\n",
    "        example = tokenize_to_features(instance)[\"examples\"]\n",
    "        return torch.tensor(example, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_to_features(entry: Dict[str, str]) -> List[int]:\n",
    "    examples = []\n",
    "    tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(entry[\"text\"]))\n",
    "    # Truncate in block of block_size\n",
    "    for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
    "        examples.append(\n",
    "            tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        remaining_text = tokenized_text[i + block_size:]\n",
    "    except UnboundLocalError:\n",
    "        if len(tokenized_text) >= block_size:\n",
    "            raise ValueError\n",
    "        remaining_text = tokenized_text\n",
    "\n",
    "    num_tokens_to_pad = block_size - len(remaining_text)\n",
    "    remaining_padded_text = remaining_text + [\n",
    "        tokenizer.eos_token_id for _ in range(num_tokens_to_pad)]\n",
    "    examples.append(remaining_padded_text)\n",
    "\n",
    "    # print(len(tokenized_text[i + block_size: ]))\n",
    "    return {\"examples\": examples, \"text\": entry[\"text\"], \"title\": entry[\"title\"]}\n",
    "\n",
    "\n",
    "def create_data_loader(dataset, batch_size, collate_fn):\n",
    "    batch = []\n",
    "    cumulative_batch_size = 0\n",
    "    for i in range(len(dataset)):\n",
    "        batch.extend([\n",
    "            x.squeeze(dim=0)\n",
    "            for x in dataset[i].split(1, dim=0)\n",
    "        ])\n",
    "        cumulative_batch_size += dataset[i].shape[0]\n",
    "        if cumulative_batch_size >= batch_size:\n",
    "            if batch_size == 1:\n",
    "                for sub_batch in batch:\n",
    "                    yield collate_fn([sub_batch])\n",
    "            else:\n",
    "                yield collate_fn(batch)\n",
    "            batch = []\n",
    "            cumulative_batch_size = 0\n",
    "            \n",
    "            \n",
    "def clip_gradient_norm_(gradients, max_norm: float = 1.0):\n",
    "    if max_norm is None:\n",
    "        return gradients\n",
    "\n",
    "    total_norm = torch.norm(torch.stack(\n",
    "        [torch.norm(grad, 2) for grad in gradients]), 2)\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "\n",
    "    for grad in gradients:\n",
    "        grad.detach().mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "tokenizer._pad_token = tokenizer.eos_token\n",
    "wrapped_dataset = LazyTextDataset(dataset)\n",
    "sampler = SequentialSampler(wrapped_dataset)\n",
    "wrapped_relevant_data = LazyTextDataset(data_collections)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False)\n",
    "relevant_data_loader = create_data_loader(\n",
    "    wrapped_relevant_data,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator)\n",
    "# data_loader = create_data_loader(\n",
    "#     wrapped_dataset,\n",
    "#     batch_size=32,\n",
    "#     collate_fn=data_collator)\n",
    "# torch.tensor(tokenize_to_features(wrapped_dataset.dataset[133353])[\"examples\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tokenize_to_features({\n",
    "    \"text\": query,\n",
    "    \"title\": None})\n",
    "\n",
    "test_input = data_collator([torch.tensor(test_input[\"examples\"], dtype=torch.long)])\n",
    "test_input[\"input_ids\"] = test_input[\"input_ids\"].view(-1, 1024)\n",
    "test_input[\"labels\"] = test_input[\"labels\"].view(-1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[44490,  3174,   318,  ..., 50256, 50256, 50256]]),\n",
       " 'labels': tensor([[44490,  3174,   318,  ...,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influence_utils.influence import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influences = compute_influences(\n",
    "#     n_gpu=1,\n",
    "#     device=torch.device(\"cuda\"),\n",
    "#     model=model,\n",
    "#     test_inputs=test_input,\n",
    "#     batch_train_data_loader=create_data_loader(\n",
    "#         wrapped_dataset,\n",
    "#         batch_size=1,\n",
    "#         collate_fn=data_collator),\n",
    "#     instance_train_data_loader=create_data_loader(\n",
    "#         wrapped_dataset,\n",
    "#         batch_size=1,\n",
    "#         collate_fn=data_collator)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62.77 | 0.00: : 1581it [10:14,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "params_filter = None\n",
    "weight_decay_ignores = None\n",
    "weight_decay = None\n",
    "weight_decay_ignores = None\n",
    "if params_filter is None:\n",
    "    params_filter = [\n",
    "        \"bert.pooler.dense.weight\",\n",
    "        \"bert.pooler.dense.bias\"]\n",
    "\n",
    "if weight_decay_ignores is None:\n",
    "    weight_decay_ignores = [\n",
    "        \"bias\",\n",
    "        \"LayerNorm.weight\"]\n",
    "\n",
    "data_loader = create_data_loader(\n",
    "        wrapped_relevant_data,\n",
    "        batch_size=1,\n",
    "        collate_fn=data_collator)\n",
    "# cache_file = os.path.join(cache_dir, \"s_test.cached.pkl\")\n",
    "# if not os.path.exists(cache_file):\n",
    "s_test = compute_s_test(\n",
    "    n_gpu=1,\n",
    "    device=torch.device(\"cuda\"),\n",
    "    model=model,\n",
    "    test_inputs=test_input,\n",
    "    train_data_loaders=[data_loader],\n",
    "    params_filter=params_filter,\n",
    "    weight_decay=weight_decay,\n",
    "    weight_decay_ignores=weight_decay_ignores,\n",
    "    scale=10000,\n",
    "    num_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(s_test, \"/export/home/Experiments/20200701/s_test.relevant.tmp\")\n",
    "# s_test = torch.load(\"/export/home/Experiments/20200701/s_test.relevant.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1581it [03:18,  7.96it/s]\n"
     ]
    }
   ],
   "source": [
    "influences = []\n",
    "train_inputs_collections = []\n",
    "for train_inputs in tqdm(relevant_data_loader):\n",
    "    grad_z = compute_gradients(\n",
    "        n_gpu=1,\n",
    "        device=torch.device(\"cuda\"),\n",
    "        model=model,\n",
    "        inputs=train_inputs,\n",
    "        params_filter=params_filter,\n",
    "        weight_decay=weight_decay,\n",
    "        weight_decay_ignores=weight_decay_ignores)\n",
    "    \n",
    "    # experimental_clip_gradient_norm_(grad_z, max_norm=1.0)\n",
    "    with torch.no_grad():\n",
    "        raise ValueError(\"Negative sign probably missing?\")\n",
    "        influence = [\n",
    "            torch.sum(x * y)\n",
    "            for x, y in zip(grad_z, s_test)]\n",
    "\n",
    "    influences.append(sum(influence).item())\n",
    "    train_inputs_collections.append(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 600,  215, 1376, 1100,   67,  132,  673,   62,  848, 1501])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(influences)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An application service provider (ASP) is a business providing computer-based services to customers over a network; such as access to a particular software application (such as customer relationship management) using a standard protocol (such as HTTP).\n",
      "\n",
      "The need for ASPs has evolved from the increasing costs of specialized software that have far exceeded the price range of small to medium-sized businesses. As well, the growing complexities of software have led to huge costs in distributing the software to end-users. Through ASPs, the complexities and costs of such software can be cut down. In addition, the issues of upgrading have been eliminated from the end-firm by placing the onus on the ASP to maintain up-to-date services, 24 x 7 technical support, physical and electronic security and in-built support for business continuity and flexible working.\n",
      "\n",
      "The importance of this marketplace is reflected by its size., estimates of the United States market ranged from 1.5 to 4 billion dollars. Clients for ASP services include businesses, government organizations, non-profits, and membership organizations.\n",
      "\n",
      "Provider types\n",
      "There are several forms of ASP business. These are:\n",
      "A specialist or functional ASP delivers a single application, such as credit card payment processing or timesheet services;\n",
      "A vertical market ASP delivers a solution package for a specific customer type, such as a dental practice;\n",
      "An enterprise ASP delivers broad spectrum solutions;\n",
      "A local ASP delivers small business services within a limited area.\n",
      "\n",
      "Some analysts identify a volume ASP as a fifth type. This is basically a specialist ASP that offers a low cost packaged solution via their own website. PayPal was an instance of this type, and their volume was one way to lower the unit cost of each transaction.\n",
      "\n",
      "In addition to these types, some large multi-line companies (such as HP and IBM), use ASP concepts as a particular business model that supports some specific customers.\n",
      "\n",
      "The ASP model\n",
      "The application software resides on the vendor's system and is accessed by users through a web browser using HTML or by special purpose client software provided by the vendor. Custom client software can also interface to these systems through XML APIs. These APIs can also be used where integration with in-house systems is required. ASPs may or may not use multitenancy in the deployment of software to clients; some ASPs offer an instance or license to each customer (for example using Virtualization), some deploy in a single instance multi-tenant access mode, now more frequently referred to as \"SaaS\".\n",
      "\n",
      "Common features associated with ASPs include:\n",
      " ASP fully owns and operates the software application(s)\n",
      " ASP owns, operates and maintains the servers that support the software\n",
      " ASP makes information available to customers via the Internet or a \"thin client\"\n",
      " ASP bills on a \"per-use\" basis or on a monthly/annual fee\n",
      "\n",
      "The advantages to this approach include:\n",
      " Software integration issues are eliminated from the client site\n",
      " Software costs for the application are spread over a number of clients\n",
      " Vendors can build more application experience than the in-house staff\n",
      " Low-code development platforms permit limited customization of pre-built applications\n",
      " Key software systems are kept up to date, available, and managed for performance by experts\n",
      " Improved reliability, availability, scalability and security of internal IT systems\n",
      " A provider's service level agreement guarantees a certain level of service\n",
      " Access to product and technology experts dedicated to available products\n",
      " Reduction of internal IT costs to a predictable monthly fee\n",
      " Redeploying IT staff and tools to focus on strategic technology projects that impact the enterprise's bottom line\n",
      "\n",
      "Some inherent disadvantages include:\n",
      " The client must generally accept the application as provided since ASPs can only afford a customized solution for the largest clients. \n",
      " The client may rely on the provider to provide a critical business function, thus limiting their control of that function and instead relying on the provider\n",
      " Changes in the ASP market may result in changes in the type or level of service available to clients\n",
      " Integration with the client's non-ASP systems may be problematic\n",
      "\n",
      "Evaluating an Application Service Provider security when moving to an ASP infrastructure can come at a high cost, as such a firm must assess the level of risk associated with the ASP itself. Failure to properly account for such risk can lead to:\n",
      " Loss of control of corporate data\n",
      " Loss of control of corporate image\n",
      " Insufficient ASP security to counter risks\n",
      " Exposure of corporate data to other ASP customers\n",
      " Compromise of corporate data\n",
      "Some other risks include failure to account for the financial future of the ASP in general, i.e. how stable a company is and if it has the resources to continue business into the foreseeable future. For these reasons Cisco Systems has developed a comprehensive evaluation guideline. This guideline includes evaluating the scope of the ASP's service, the security of the program and the ASP's maturity with regard to security awareness. Finally the guidelines indicate the importance of performing audits on the ASP with respect to:\n",
      " Port/Network service\n",
      " Application vulnerability\n",
      " ASP Personnel\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_inputs_collections[848][\"input_ids\"].cpu().detach().numpy().squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Salesforce is a tech company with headquarter in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "137\n",
      "315\n",
      "317\n",
      "398\n",
      "403\n",
      "595\n"
     ]
    }
   ],
   "source": [
    "ds = []\n",
    "for i, d in enumerate(data_collections):\n",
    "    if (\"salesforce\" in d[\"text\"].lower() and \"california\" in d[\"text\"].lower() and \"headquarter\" in d[\"text\"].lower() and \"tech\" in d[\"text\"].lower()\n",
    "        and \"company\" in d[\"text\"].lower()):\n",
    "        ds.append((i, d))\n",
    "    \n",
    "    if \"salesforce\" in d[\"title\"].lower():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(influences)[403]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 50]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(ds)) if \"salesforce\" in ds[i][1][\"title\"].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
