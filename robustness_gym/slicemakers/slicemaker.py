from __future__ import annotations

from itertools import compress
from typing import *

import cytoolz as tz
import numpy as np

from robustness_gym.cached_ops.cached_ops import CachedOperation
from robustness_gym.constants import *
from robustness_gym.dataset import Dataset
from robustness_gym.identifier import Identifier
from robustness_gym.slice import Slice
from robustness_gym.storage import PicklerMixin
from robustness_gym.tools import recmerge


class SliceMaker(PicklerMixin):
    CATEGORIES = [
        SUBPOPULATION,
        ATTACK,
        AUGMENTATION,
        CURATION,
    ]

    def __init__(self,
                 category: str,
                 identifiers: List[Identifier],
                 apply_fn: Callable = None,
                 *args,
                 **kwargs):

        super(SliceMaker, self).__init__(*args, **kwargs)

        # The SliceMaker belongs to a category
        assert category in self.CATEGORIES, f"argument category must be one of {self.CATEGORIES}"
        self.category = category

        # Each identifier corresponds to a single output Slice generated by this SliceMaker
        self.num_slices = len(identifiers)
        self.identifiers = identifiers

        self.tasks = None

        self.metadata = {}

        # Keep track of the CachedOperation dependencies
        self.prerequisites = set()
        for base in self.__class__.__bases__:
            for cls in base.__mro__:
                if str(CachedOperation.__name__) in str(cls):
                    self.prerequisites.add(base)

        if apply_fn:
            # Assign to the method
            self.apply = apply_fn

    def __call__(self,
                 batch_or_dataset: Union[Dict[str, List], Dataset],
                 keys: List[str],
                 mask: List[int] = None,
                 store_compressed: bool = None,
                 store: bool = None,
                 *args,
                 **kwargs):

        # Check that prerequisites are satisfied
        self.prerequisites_handler(batch_or_dataset)

        if isinstance(batch_or_dataset, Dataset):

            # Slice a dataset
            dataset, slices, slice_membership = self.process_dataset(
                dataset=batch_or_dataset,
                keys=keys,
                # Automatically infer the mask from the Dataset if it's not specified
                mask=[batch_or_dataset.check_tape(
                    path=[SLICEMAKERS, self.category],
                    identifier=identifier,
                    keys=keys
                )
                    for identifier in self.identifiers
                ] if not mask else mask,
                store_compressed=True if store_compressed is None else store_compressed,
                store=True if store is None else store,
                *args,
                **kwargs
            )

            # Update the Dataset's history
            for identifier in self.identifiers:
                dataset.update_tape(
                    path=[SLICEMAKERS, self.category],
                    identifier=identifier,
                    keys=keys,
                )

            return dataset, slices, slice_membership

        elif isinstance(batch_or_dataset, Dict):
            if store_compressed is True:
                print("Compressed storage cannot be used on a batch. Please use Dataset.from_batch(batch) before "
                      "applying the SliceMaker.")
            # Slice a batch
            return self.process_batch(
                batch=batch_or_dataset,
                keys=keys,
                mask=mask,
                # Don't allow compressed storage for __call__ on a batch
                store_compressed=False,
                # Don't store by default
                store=False if store is None else store,
                *args,
                **kwargs
            )
        else:
            raise NotImplementedError

    def prerequisites_handler(self,
                              batch_or_dataset: Union[Dict[str, List], Dataset]):
        if isinstance(batch_or_dataset, Dataset):
            batch = batch_or_dataset[:2]
        else:
            batch = batch_or_dataset

        # Check if pre-requisites are satisfied
        # TODO(karan): move to a method
        if 'cache' not in batch:
            pending = self.prerequisites
        else:
            pending = {prerequisite for prerequisite in self.prerequisites
                       if not prerequisite.available(batch)}

        # TODO(karan): Automatically run the pending pre-requisites
        if pending:
            raise RuntimeError(f"Cannot run SliceMaker, prerequisites {pending} not satisfied.")

    @staticmethod
    def store(batch: Dict[str, List],
              updates: List[Dict]) -> Dict[str, List]:
        """
        Update a batch of examples with slice information.
        """
        if 'slices' not in batch:
            batch['slices'] = [{} for _ in range(len(batch['index']))]

        # For each example, recursively merge the example's original cache dictionary with the update dictionary
        batch['slices'] = [
            recmerge(example_dict, update_dict, merge_sequences=True)
            for example_dict, update_dict in zip(batch['slices'], updates)
        ]

        return batch

    def process_dataset(self,
                        dataset: Dataset,
                        keys: List[str],
                        batch_size: int = 32,
                        mask: List[int] = None,
                        store_compressed: bool = True,
                        store: bool = True,
                        *args,
                        **kwargs) -> Tuple[Dataset, List[Slice], np.ndarray]:
        """
        Apply a SliceMaker to a dataset.
        """

        # Batch the dataset, and slice each batch
        all_batches, all_sliced_batches, all_slice_memberships = zip(
            *[self.process_batch(batch=tz.merge_with(tz.identity, *batch),
                                 keys=keys,
                                 mask=mask,
                                 store_compressed=store_compressed,
                                 store=store,
                                 *args,
                                 **kwargs)
              for batch in tz.partition_all(batch_size, dataset)]
        )

        # TODO(karan): want to do this instead but .map() in Huggingface nlp must return either a None type or dict
        # all_batches, all_sliced_batches, all_slice_memberships = \
        #     zip(*dataset.map(lambda examples: self.slice_batch(batch=examples, keys=keys),
        #                      batched=True, batch_size=batch_size))

        # Update the dataset efficiently by reusing all_batches
        # TODO(karan): have to run this separately since .map() only updates the dataset if a dict is returned
        dataset = dataset.map(lambda examples, indices: all_batches[indices[0] // batch_size],
                              batched=True, batch_size=batch_size, with_indices=True)

        # Create the dataset slices
        slices = [Slice.from_batches(slice_batches)
                  for slice_batches in zip(*all_sliced_batches)]
        # TODO(karan): make this more systematic
        for i, sl in enumerate(slices):
            # TODO(karan): fix the combination of identifiers
            sl.identifier = str(dataset.identifier) + '-' + str(self.identifiers[i])
            sl.info.features = dataset.features

        # Create a single slice label matrix
        slice_membership = np.concatenate(all_slice_memberships, axis=0)

        return dataset, slices, slice_membership

    def process_batch(self,
                      batch: Dict[str, List],
                      keys: List[str],
                      mask: List[int] = None,
                      store_compressed: bool = True,
                      store: bool = True,
                      *args,
                      **kwargs) \
            -> Tuple[Dict[str, List], List[Dict[str, List]], Optional[np.ndarray]]:
        return batch, [batch], None

    def apply(self, *args, **kwargs):
        raise NotImplementedError("Must implement apply.")

    @classmethod
    def join(cls, *slicemakers: SliceMaker) -> Sequence[SliceMaker]:
        """
        Join many slicemakers. By default, just returns the slicemakers.
        """
        return slicemakers

    def masked(self, mask: List[int]):
        pass

    def unmasked(self):
        pass

    @staticmethod
    def filter_batch_by_slice_membership(batch: Dict[str, List],
                                         slice_membership: np.ndarray) -> List[Dict[str, List]]:
        """
        Use a matrix of slice membership labels to select the subset of examples in each slice.

        Returns a list. Each element in the list corresponds to a single slice, and
        contains the subset of examples in 'batch' that lies in that slice.
        """
        return [tz.valmap(lambda v: list(compress(v, s)), batch) for s in slice_membership.T]
